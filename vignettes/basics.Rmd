---
title: "Basics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(pcs.sim.pkg)

# create a data-generating process function to pass to create_dgp()
dgp_fun <- function(n, noise_level = 4) {
  X <- matrix(rnorm(n * 2), nrow=n)
  y <- cbind(1, X) %*% c(-8, 3, -1) + rnorm(n, sd = noise_level)
  return(list(X = X, y = y))
}

dgp <- create_dgp(dgp_fun, n = 200, noise_level = 1)

# create a method function to pass to create_method(); this this case, just get
# the p-values of the two predictors in the OLS fit of the data
method_fun <- function(X, y) {
  fit <- lm(y ~ X)
  return(list(
    "X1 p-val" = summary(fit)$coefficients["X1", "Pr(>|t|)"],
    "X2 p-val" = summary(fit)$coefficients["X2", "Pr(>|t|)"]
  ))
}

method <- create_method(method_fun)

eval_summary_fun <- function(fit_results) {
  eval_out <- fit_results %>%
    dplyr::group_by(dgp, method) %>%
    dplyr::summarise_all(~mean(.x < 0.05))
  return(eval_out)
}

eval_fun <- function(fit_results) {
  eval_out <- fit_results %>%
    dplyr::mutate_at(dplyr::vars(-dgp, -method), 
                     ~ifelse(.x < 0.05, "reject", "fail to reject"))
  return(eval_out)
}

summary_evaluator <- create_evaluator(eval_summary_fun)
evaluator <- create_evaluator(eval_fun)

power_plot <- function(fit_results, eval_results = NULL) {
  require(ggplot2)
  plt <- ggplot(fit_results) +
    aes(x = `X1 p-val`, color = as.factor(method)) +
    geom_abline(slope = 1, intercept = 0, 
                color = "darkgray", linetype = "solid", size = 1) +
    stat_ecdf(size = 1) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x = "t", y = "P( p-value \u2264 t )", linetype = "", color = "Method")
  
  return(plt)
}

plotter <- create_plotter(power_plot)

# create the experiment and add the dgp and method
experiment <- create_experiment(name = "base") %>%
  add_dgp(dgp) %>%
  add_method(method) %>%
  add_evaluator(summary_evaluator) %>%
  add_evaluator(evaluator) %>%
  add_plotter(plotter)

future::plan(future::sequential)

# run sequentually
seq_start <- proc.time()
o1 <- experiment$fit(n_reps = 100)
proc.time() - seq_start

eval_res <- experiment$evaluate(o1)

plot_res <- experiment$plot(o1, eval_res)

# future::plan(future::multiprocess, workers=2)
# 
# # run in parallel with 2 processes
# par_start <- proc.time()
# o2 <- experiment$run(n_reps = 100)
# proc.time() - par_start
# 
# future::plan(future::sequential)
```

# Inheriting traits from an existing Experiment

```{r inheritance}
clone <- create_experiment(
  clone_from = experiment, name = "cloned"
)

# a copy of dgps/method/evaluators is made when inheriting
data.table::address(experiment$get_dgps()[["dgp1"]]) !=
  data.table::address(clone$get_dgps()[["dgp1"]]) # FALSE

# the clone can have additional dgps (or methods/evaluators) beyond what the parent has
dgp2 <- create_dgp(dgp_fun, n = 500, noise_level = 3)
clone$add_dgp(dgp2)

names(experiment$get_dgps())
names(clone$get_dgps())

# the cloned experiment can override its parent's dgps without modifying the parent
dgp3 <- create_dgp(dgp_fun, n = 10000, noise_level = 10)
clone$update_dgp(dgp3, "dgp1")

identical(experiment$get_dgps()[["dgp1"]], clone$get_dgps()[["dgp1"]]) # FALSE

# inheritance is nested...
clone2 <- create_experiment(n_reps = 100, clone_from = clone, name = "another clone")
names(clone2$get_dgps())

# ...and allows for parallel branches
clone3 <- create_experiment(n_reps = 100, clone_from = experiment, name = "clone3")
names(clone3$get_dgps())
```
