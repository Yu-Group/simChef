---
title: "simChef Vignette"
# author: "James Duncan, Corrine Elliott, Tiffany Tang"
# output: rmarkdown::html_vignette
output: 
  prettydoc::html_pretty:
    theme: architect
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{simChef Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(pcs.sim.pkg)
```

# Overview

The goal of `simChef` is to seamlessly and efficiently run simulation experiments using a simple grammar. The results of these simulation experiments can also be conveniently viewed in an organized and interactive browser (or html file). The basic usage of `simChef` can be summarized as follows:

1. Create individual parts of the simulation experiment recipe
    a. Create DGP(s) (data-generating processes) via `create_dgp()`
    b. Create Method(s) via `create_method()`
    c. Create Evaluator(s) via `create_evaluator()`
    d. Create Plotter(s) via `create_plotter()`
2. Assemble recipe parts into a complete simulation experiment recipe, e.g., via
```{r eval = FALSE}
experiment <- create_experiment(name = "Experiment") %>%
  add_dgp(dgp, name = "DGP1") %>%
  add_method(method, name = "Method1") %>%
  add_evaluator(evaluator, name = "Evaluator1") %>%
  add_plotter(plotter, name = "Plotter1")
```
3. Run the experiment
```{r eval = FALSE}
results <- experiment$run(n_reps = 100, save = T)  
# or alternatively, `run_experiment(experiment, n_reps = 100, save = T)`
```
4. Document simulation experiment recipe and visualize results via R Markdown
```{r eval = FALSE}
experiment$create_rmd()  
# or alternatively, `create_rmd(experiment)`
```

We will next go through a toy example simulation using linear regression.

# Create individual parts of the simulation experiment recipe

To begin, we first need to create the individual parts of the simulation experiment recipe. There are four main components/classes and verbs in the simulation experiment:

1. **DGP** (data-generating process): the data-generating processes from which to **generate** data
2. **Method**: the methods (or models) to **fit** in the experiment
3. **Evaluator**: the evaluation metrics used to **evaluate** the methods performance
4. **Plotter**: the plotting functions used to **plot** outputs from the method fits or evaluation results

To create a `DGP`, `Method`, `Evaluator`, or `Plotter`, we can respectively use the `create_dgp()`, `create_method()`, `create_evaluator()`, or `create_plotter()` functions. Each `create_*` function follows the same syntax and takes in the inputs:

- `*_fun`: the first input is a function from which to simulate data, fit the method, evaluate the metrics, or create the plots (depending on `*`)
- `...`: additional arguments get passed into the `*_fun` above

## Create data-generating process (DGP)

As a toy DGP example, let us create a function to simulate a random Gaussian data matrix $\mathbf{X}$ of size $n \times 2$ and a linear response vector $\mathbf{y}$ of size $n \times 1$, where

\begin{gather*}
\mathbf{X} \sim N\left(\mathbf{0}, \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}\right), \\
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},\\
\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I}_n)
\end{gather*}

```{r} 
dgp_fun <- function(n, beta, rho, sigma) {
  cov_mat <- matrix(c(1, rho, rho, 1), byrow = T, nrow = 2, ncol = 2)
  X <- MASS::mvrnorm(n = n, mu = rep(0, 2), Sigma = cov_mat)
  y <- X %*% beta + rnorm(n, sd = sigma)
  return(list(X = X, y = y))
}
```

We can create an object of class `DGP` from this function via

```{r}
dgp <- create_dgp(dgp_fun = dgp_fun, 
                  # additional named parameters to pass to dgp_fun()
                  n = 200, beta = c(1, 0), rho = 0, sigma = 1)
```

Note that the additional arguments in `create_dgp` must be named arguments that match those in `dgp_fun`. 

## Create method

Given the above DGP, suppose we want to investigate the performance of linear regression, specifically the p-values that are outputted for the non-intercept coefficients from `summary.lm()`.

```{r}
lm_fun <- function(X, y, cols = c("X1", "X2")) {
  lm_fit <- lm(y ~ X)
  pvals <- summary(lm_fit)$coefficients[cols, "Pr(>|t|)"] %>%
    setNames(paste(names(.), "p-value"))
  return(pvals)
}
```

We can create an object of class `Method` from this function via

```{r}
lm_method <- create_method(method_fun = lm_fun)
```

A couple notes here:

- The inputs into the method function `lm_fun` should include the named outputs from the DGP function `dgp_fun` (in this case, `X` and `y`), as the data generated from the DGP will be automatically passed into the Method.
- Additional arguments can be passed into `create_method()` as was done previously in `create_dgp()`. However, this is not necessary in this case if we are happy with the default argument for `cols` in `lm_fun()`.

## Create evaluator

To evaluate the performance of linear regression, one metric (or statistic) of interest could be the rejection probability at some level $\alpha$, which we compute in the following function.

```{r}
reject_prob_fun <- function(fit_results, alpha = 0.05) {
  group_vars <- c("dgp", "method")
  eval_out <- fit_results %>%
    dplyr::group_by(across({{group_vars}})) %>%
    dplyr::summarise(
      `X1 Reject Prob.` = mean(`X1 p-value` < alpha),
      `X2 Reject Prob.` = mean(`X2 p-value` < alpha)
    )
  return(eval_out)
}
```

We can create an object of class `Evaluator` from this function via

```{r}
reject_prob_eval <- create_evaluator(eval_fun = reject_prob_fun, alpha = 0.1)
```

There are a few key points to keep in mind when writing a custom Evaluator function such as `reject_prob_fun()`.

- First, the Evaluator function should almost always take in the named argument `fit_results`. `fit_results` is a placeholder for the tibble that will be outputted by `Experiment$fit()` and automatically passed in when the experiment is fitted. Note that this argument must be exactly named `fit_results` or else the Evaluator function will not receive the results from `Experiment$fit()`. We will see `Experiment$fit()` in action later, but for now, we can think of `fit_results` as a tibble containing the results of all (repetition, DGP, method) combinations fitted in the experiment. Naturally, `fit_results` will have columns named `rep`, `dgp`, `method`, and any other named arguments that were outputted from the method function (i.e., `lm_fun`).
- Many evaluation metrics are most naturally computed across repetitions, so it is common to group the `fit_results` by `dgp` and `method` as seen in `reject_prob_fun()` above. In doing so, the rejection probability (across repetitions) will be computed for each (DGP, Method) combination separately. However, depending on the goal of the Evaluator function, grouping by `dgp` and `method` might not be necessary.

As before, additional arguments can be passed into `create_evaluator()`, and in this case, we have overwritten the default value of `alpha` with `alpha = 0.1`.

## Create plotter

Lastly, we may want to plot the results from the `Method` fits (stored in `fit_results`) and/or the outputs from our `Evaluators` (stored in `eval_results`). For example, we could plot the power of the hypothesis test.

```{r}
power_plot_fun <- function(fit_results, col = "X1") {
  plt <- ggplot2::ggplot(fit_results) +
    ggplot2::aes(x = .data[[paste(col, "p-value")]],
                 color = as.factor(method)) +
    ggplot2::geom_abline(slope = 1, intercept = 0, 
                         color = "darkgray", linetype = "solid", size = 1) +
    ggplot2::stat_ecdf(size = 1) +
    ggplot2::scale_x_continuous(limits = c(0, 1)) +
    ggplot2::labs(x = "t", y = "P( p-value \u2264 t )", 
                  linetype = "", color = "Method")
  return(plt)
}
```

We can create an object of class `Plotter` from this function via

```{r}
power_plot <- create_plotter(plot_fun = power_plot_fun)
```

Like the custom `Evaluator` functions, custom `Plotter` functions such as `power_plot_fun()` can take in the argument `fit_results` if we want to construct a plot from the method fit outputs. If we want to construct a plot using the output of our `Evaluators` (e.g., the output from `reject_prob_fun()`), the `Plotter` function can also take the argument `eval_results` as input. Note again that the arguments must be exactly named `fit_results` and `eval_results` in order to work properly.

As with the other `create_*` functions, additional arguments that need to pass into the custom `Plotter` function can be passed into `create_plotter()`.

# Assemble simulation experiment recipe

At this point, we have created a `DGP` (`dgp`), `Method` (`lm_method`), `Evaluator` (`reject_prob_eval`), and `Plotter` (`power_plot`). The next step is to create the simulation experiment recipe and add each component to the recipe via

```{r}
experiment <- create_experiment(name = "Linear Regression Experiment") %>%
  add_dgp(dgp, name = "Linear Gaussian DGP") %>%
  add_method(lm_method, name = "OLS") %>%
  add_evaluator(reject_prob_eval, name = "Rejection Prob. (alpha = 0.1)") %>%
  add_plotter(power_plot, name = "Power")
```

Any number of DGPs, Methods, Evaluators, and Plotters can be added to the simulation experiment recipe, but each added element to the experiment should have an intelligible name as this will be used in creating the R Markdown results report.

We can easily see the individual parts of the simulation experiment recipe by printing the experiment.

```{r results = "hide"}
print(experiment)
#> Experiment Name: Linear Regression Experiment 
#>    Saved results at: ./results/Linear Regression Experiment 
#>    DGPs: Linear Gaussian DGP 
#>    Methods: OLS 
#>    Evaluators: Rejection Prob. (alpha = 0.1) 
#>    Plotters: Power 
#>    Vary Across: None
```

# Run the experiment

Thus far, we have only created the simulation experiment recipe and have not generated any results from the experiment. That is, we have only given the simulation experiment instructions on what to do. To run the experiment, say over 100 repetitions, we can do so via

```{r}
results <- experiment$run(n_reps = 100, save = TRUE)
```

or alternatively,

```{r eval = FALSE}
results <- experiment %>%
  run_experiment(n_reps = 100, save = TRUE)
```

The output of `experiment$run()` is a list of length three:

- `fit_results`: output of `Method` fits (see `experiment$fit()`)
- `eval_results`: output of `Evaluators` (see `experiment$evaluate()`)
- `plot_results`: output of `Plotters` (see `experiment$plot()`)

```{r}
str(results, max.level = 2)
results$fit_results
results$eval_results
results$plot_results
```


By default, the results are not saved to disk. However, to generate the R Markdown report, we will need to save the results to disk and hence set `save = TRUE` above.

The experiment can also be run in parallel. For a more detailed walkthrough on how to parallelize the experiment, please see the [NAME OF VIGNETTE]().

# Document and visualize results

Finally, we can easily visualize all results from the simulation experiment in an html file (generated using R Markdown) or browser.

```{r}
experiment$create_rmd(open = FALSE)
```

or alternatively,

```{r eval = FALSE}
create_rmd(experiment, open = FALSE)
```

The results can be found [here](). 

Note that in the first run of `experiment$create_rmd()`, a documentation template (i.e., a series of blank .md files) is created for the user to fill out with descriptions of the simulation experiment and its recipe components. These blank .md files can be found in the experiment's root results directory under docs/. To find the experiment's root results directory, use `experiment$get_save_dir()`.

# Advanced usage

## Run experiment across varying parameters in the DGP/Method

In many situations, it is helpful to understand how a method's performance is affected as we vary a single parameter in the DGP across different values. For instance, what happens to the power as the amount of noise in the linear model increases?

Using the simple grammar of `simChef`, we can investigate this question by adding a "vary across" component to the simulation experiment.
```{r}
experiment <- experiment %>%
  add_vary_across(dgp = "Linear Gaussian DGP", param_name = "sigma",
                  param_values = c(1, 2, 4, 8))
```

```{r results = "hide"}
print(experiment)
#> Experiment Name: Linear Regression Experiment 
#>    Saved results at: ./results/Linear Regression Experiment 
#>    DGPs: Linear Gaussian DGP 
#>    Methods: OLS 
#>    Evaluators: Rejection Prob. (alpha = 0.1) 
#>    Plotters: Power 
#>    Vary Across: 
#>       DGP: Linear Gaussian DGP 
#>       Parameter: sigma 
#>       Parameter values:  num [1:4] 1 2 4 8
```


In `add_vary_across`, the `dgp` argument is the name of the DGP to vary or the DGP object itself. The `param_name` argument is the name of the argument/parameter in the `DGP` function that will be varied, and the `param_values` argument is a list or atomic vector of values that `param_name` will take and vary across while all other arguments are kept constant at their base value (see `dgp$dgp_params`). Here, we are varying the `sigma` parameter (i.e., the SD of the additive noise term) over values of 1, 2, 4, and 8 within the Linear Gaussian DGP. (Note: we can also vary across parameters in a `Method` by inputting the `method` argument instead of the `dgp` argument in `add_vary_across()`.)

However, when we run the experiment, the results are not quite what we expect. The results are summarized/aggregated across all values of `sigma`.

```{r}
vary_results <- experiment$run(n_reps = 100, save = TRUE)
vary_results$eval_results
vary_results$plot_results
```

To see how different values of `sigma` affect the experiment results, we need to modify our `Evaluator` and `Plotter` functions. Specifically, in `reject_prob_eval`, we want to group `fit_results` by `sigma` in addition to `dgp` and `method`. To do this, we need to add a `vary_param` argument to our custom `Evaluator` function. `vary_param` will be auto-filled by the `param_name` argument from `add_vary_across()` (in this case, `sigma`) when we run the experiment.

```{r}
reject_prob_fun <- function(fit_results, vary_param = NULL, alpha = 0.05) {
  group_vars <- c("dgp", "method", vary_param)
  eval_out <- fit_results %>%
    dplyr::group_by(across({{group_vars}})) %>%
    dplyr::summarise(
      `X1 Reject Prob.` = mean(`X1 p-value` < alpha),
      `X2 Reject Prob.` = mean(`X2 p-value` < alpha)
    )
  return(eval_out)
}
reject_prob_eval <- create_evaluator(eval_fun = reject_prob_fun, alpha = 0.1)
```

Similarly in `power_plot_fun`, we need to add a `vary_param` argument to plot the results across different values of `sigma`.

```{r}
power_plot_fun <- function(fit_results, vary_param = NULL, col = "X1") {
  plt <- ggplot2::ggplot(fit_results) +
    ggplot2::aes(x = .data[[paste(col, "p-value")]],
                 color = as.factor(method)) +
    ggplot2::geom_abline(slope = 1, intercept = 0, 
                         color = "darkgray", linetype = "solid", size = 1) +
    ggplot2::stat_ecdf(size = 1) +
    ggplot2::scale_x_continuous(limits = c(0, 1)) +
    ggplot2::labs(x = "t", y = "P( p-value \u2264 t )", 
                  linetype = "", color = "Method")
  if (!is.null(vary_param)) {
    plt <- plt + ggplot2::facet_wrap(~ .data[[vary_param]])
  }
  return(plt)
}
power_plot <- create_plotter(plot_fun = power_plot_fun)

```

Now, we are ready to update our experiment and run the experiment via
```{r}
vary_results <- experiment %>%
  update_evaluator(reject_prob_eval, name = "Rejection Prob. (alpha = 0.1)") %>%
  update_plotter(power_plot, name = "Power") %>%
  run_experiment(n_reps = 100, save = TRUE)
vary_results$eval_results
vary_results$plot_results
```

Note here we need to use `update_*` instead of `add_*` since an `Evaluator` named "Rejection Prob. (alpha = 0.1)" and a `Plotter` named "Power" already exist in the  `Experiment`. Using `add_*` will throw an error.

For fun, let's add another plot (in fact, an interactive plot using `plotly::ggplotly`) to our `Experiment` and run the `Experiment` across various values of the coefficient $\boldsymbol{\beta}_2$ and the correlation $\rho$ between features in $\mathbf{X}$. (Note: the plotter function below (`reject_prob_plot_fun`) takes in the `Evaluator` results, stored as `eval_results`, and plots the rejection probability for the $\boldsymbol{\beta}_1$ at $\alpha = 0.1$.)

```{r}
# create rejection probability plot
reject_prob_plot_fun <- function(eval_results, vary_param = NULL, 
                                 alpha = 0.05) {
  plt <- ggplot2::ggplot(eval_results$`Rejection Prob. (alpha = 0.1)`) +
    ggplot2::aes(x = .data[[vary_param]], y = `X1 Reject Prob.`, 
                 color = as.factor(method)) +
    ggplot2::labs(x = vary_param, 
                  y = sprintf("Rejection Probability (alpha = %s)", alpha),
                  color = "Method") +
    ggplot2::geom_line() +
    ggplot2::geom_point(size = 2) +
    ggplot2::scale_y_continuous(limits = c(0, 1))
  return(plotly::ggplotly(plt))
}
reject_prob_plot <- create_plotter(plot_fun = reject_prob_plot_fun, alpha = 0.1)

# run experiment across values of beta_2
vary_beta2_results <- experiment %>%
  update_vary_across(dgp = "Linear Gaussian DGP", param_name = "beta",
                     param_values = list(c(1, 0),
                                         c(1, 0.5),
                                         c(1, 1),
                                         c(1, 1.5),
                                         c(1, 2))) %>%
  run_experiment(n_reps = 100, save = TRUE)

# run experiment across values of rho (correlation)
vary_cor_results <- experiment %>%
  update_vary_across(dgp = "Linear Gaussian DGP", param_name = "rho", 
                     param_values = c(0, 0.2, 0.5, 0.9)) %>%
  run_experiment(n_reps = 100, save = TRUE)
```
Note that while multiple DGPs, Methods, Evaluators, and Plotters can be added to the `Experiment`, only one `vary_across` component can be added to the `Experiment` at this time.

## Experiment as an R6 Class

Since the `Experiment` class is an `R6`, we need to be careful about clones versus pointers. In the following, it may look like the `vary_experiment` object has a `vary_across` component while the `experiment` object does not have a `vary_across` component. However, when `experiment` is piped into `add_vary_across()`, this is in itself modifying `experiment`, and `vary_experiment` is simply pointing to this modified `experiment`.

```{r results = "hide"}
experiment <- experiment %>%
  remove_vary_across()
experiment
#> Experiment Name: Linear Regression Experiment 
#>    Saved results at: ./results/Linear Regression Experiment 
#>    DGPs: Linear Gaussian DGP 
#>    Methods: OLS 
#>    Evaluators: Rejection Prob. (alpha = 0.1) 
#>    Plotters: Power 
#>    Vary Across: None
vary_experiment <- experiment %>%
  add_vary_across(dgp = "Linear Gaussian DGP", param_name = "sigma",
                  param_values = c(1, 2, 4, 8))
all.equal(vary_experiment, experiment)
#> [1] TRUE
data.table::address(experiment) == data.table::address(vary_experiment)
#> [1] TRUE
experiment
#> Experiment Name: Linear Regression Experiment 
#>    Saved results at: ./results/Linear Regression Experiment 
#>    DGPs: Linear Gaussian DGP 
#>    Methods: OLS 
#>    Evaluators: Rejection Prob. (alpha = 0.1) 
#>    Plotters: Power 
#>    Vary Across: 
#>       DGP: Linear Gaussian DGP 
#>       Parameter: sigma 
#>       Parameter values:  num [1:4] 1 2 4 8

```

## Creating an experiment from another experiment

To modify `vary_experiment` without making changes to `experiment`, we need to create a new experiment by explicitly *cloning* from the old experiment.

```{r}
vary_experiment <- create_experiment(name = "I am a clone",
                                     clone_from = experiment)
data.table::address(experiment) == data.table::address(vary_experiment)
```

```{r results = "hide"}
vary_experiment
#> Experiment Name: I am a clone 
#>    Saved results at: ./results/Linear Regression Experiment 
#>    DGPs: Linear Gaussian DGP 
#>    Methods: OLS 
#>    Evaluators: Rejection Prob. (alpha = 0.1) 
#>    Plotters: Power 
#>    Vary Across: None
```

When creating an `Experiment` from a clone, we are making a deep clone of the parent experiment's `DGPs`, `Methods`, `Evaluators`, and `Plotters`, but not the `vary_across` component. We thus need to add a `vary_across` component to `vary_experiment` using `add_vary_across()`.

```{r}
# this works without an error
vary_experiment <- vary_experiment %>%
  add_vary_across(dgp = "Linear Gaussian DGP", param_name = "sigma",
                  param_values = c(1, 2, 4, 8))
```

We can also add/update DGPs, Methods, Evaluators, and Plotters to the cloned experiment without modifying the parent `experiment`.

```{r results = "hide"}
# add DGP
dgp_new <- create_dgp(dgp_fun = dgp_fun, 
                      n = 500, beta = c(1, 0), rho = 0, sigma = 1)
vary_experiment %>%
  add_dgp(dgp_new, "Linear Gaussian DGP (large n)")
#> Experiment Name: I am a clone 
#>    Saved results at: /Users/Tiffany/My Documents/Research/Yu/PCS Theory/pcs-sim-pkg/vignettes/results/Linear Regression Experiment 
#>    DGPs: Linear Gaussian DGP, Linear Gaussian DGP (large n) 
#>    Methods: OLS 
#>    Evaluators: Rejection Prob. (alpha = 0.1) 
#>    Plotters: Power 
#>    Vary Across: 
#>       DGP: Linear Gaussian DGP 
#>       Parameter: sigma 
#>       Parameter values:  num [1:4] 1 2 4 8
experiment
#> Experiment Name: Linear Regression Experiment 
#>    Saved results at: ./results/Linear Regression Experiment 
#>    DGPs: Linear Gaussian DGP 
#>    Methods: OLS 
#>    Evaluators: Rejection Prob. (alpha = 0.1) 
#>    Plotters: Power 
#>    Vary Across: 
#>       DGP: Linear Gaussian DGP 
#>       Parameter: sigma 
#>       Parameter values:  num [1:4] 1 2 4 8
```

## Additional R Markdown notes and options

When generating the R Markdown report summary for an `Experiment`, the R Markdown will compile results (evaluation and plot results) from all saved `Experiments` under the root results directory `experiment$get_save_dir()`. Since the results from the many `vary_across` runs above are all saved under the original `experiment`'s results directory, then the following will include all of these results in a single document.

```{r}
experiment$create_rmd(open = FALSE)
```

We can also create the R Markdown report summary by directly specifying the experiment's root results directory.

```{r eval = FALSE}
create_rmd(experiment_dirname = experiment$get_save_dir(), open = FALSE)
```

The results can be found [here](). 

In addition to showing all results under the root results directory, `create_rmd()` will automatically generate a blank documentation template for every `DGP`, `Method`, `Evaluator`, and `Plot` found in any one of the `Experiments` under the root results directory. If one would like to generate the documentation template but not create the R Markdown report, see `create_doc_template()`.

There are also several customizable options regarding the aesthetics of the R Markdown output. These can be modified using the `rmd_options` argument in `create_evaluator()` and `create_plotter()`. For example, we can customize the height and width of the Power plot via
```{r}
power_plot <- create_plotter(plot_fun = power_plot_fun, 
                             rmd_options = list(height = 10, width = 8))
```

and the number of digits in the evaluation table outputs via
```{r}
reject_prob_eval <- create_evaluator(eval_fun = reject_prob_fun, alpha = 0.1,
                                     rmd_options = list(digits = 3))
```

## Saving results

Since the R Markdown report heavily relies on the results file structure to organize the report summary, it may be helpful to understand the default saving structure utilized in `simChef`.

By default, the root results directory is ./results/\{EXPERIMENT_NAME\}. If the experiment was created by cloning another experiment, then the default results directory is the same as the parent experiment. To change the root results directory, one can specify the desired directory via the `save_dir` argument in `create_experiment()` or use the `set_save_dir()` helper function.

All results from `experiment$run(..., save = TRUE)` without a `vary_across` component are then saved under the root results directory. If `experiment` has a `vary_across` component, then the results of `experiment$run(..., save = TRUE)` are saved under ./\{ROOT_RESULTS_DIR\}/\{DGP_OR_METHOD_NAME\}/Varying \{PARAM_NAME\}/.

## Additional handling of the experiment

`Experiment$run()` is the easiest and most concise way of running the simulation experiment from start to finish. However, when debugging and developing the simulation experiment, it may be helpful to run only parts of the experiment so as to not repeat time-consuming, redundant computations. We split up the experimental run into the following three parts:

1. `experiment$fit()`: fit the Method(s) in the experiment on multiple repetitions of the DGP(s) and return the results from the fits
2. `experiment$evaluate()`: evaluate the experiment through the Evaluator(s) and return the evaluation results
3. `experiment$plot()`: plot visualizations of the fit/evaluation results from the experiment using the Plotter(s) and return the plot results

```{r}
fit_results <- experiment$fit(n_reps = 100)
eval_results <- experiment$evaluate(fit_results)
plot_results <- experiment$plot(fit_results, eval_results)
```

or alternatively,

```{r eval = FALSE}
fit_results <- fit_experiment(experiment, n_reps = 100)
eval_results <- evaluate_experiment(experiment, fit_results)
plot_results <- plot_experiment(experiment, fit_results, eval_results)
```

`Experiment$run()` is simply a wrapper around these three functions: `fit()`, `evaluate()`, and `plot()`.

If the results have been saved to disk previously, and one would like to read in these saved results instead of re-running the experiment, the `use_cached` argument should be set to `TRUE`.

Thus far, we have neither stored nor returned any data from the `DGPs` since these can be large objects that require high memory loads when `n_reps` is large. However, one can generate small samples of data from the `DGPs` in the experiment via

```{r}
data_list <- experiment$generate_data(n_reps = 1)
str(data_list)
```

or alternatively,

```{r eval = FALSE}
data_list <- generate_data(experiment, n_reps = 1)
```

This might be helpful for exploratory data analysis or further diagnosis of the experiment results.

Other helpful methods for handling the experiment include the `get_*` family of methods, i.e., 
```{r}
experiment$get_dgps()
experiment$get_methods()
experiment$get_evaluators()
experiment$get_plotters()
experiment$get_vary_across()
```

```{r results = "hide"}
experiment$get_save_dir()
#> [1] "./results/Linear Regression Experiment"
```


