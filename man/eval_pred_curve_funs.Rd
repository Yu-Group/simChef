% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluator-lib-prediction.R
\name{eval_pred_curve_funs}
\alias{eval_pred_curve_funs}
\alias{eval_pred_curve}
\alias{summarize_pred_curve}
\title{Evaluate and/or summarize ROC or PR curves.}
\usage{
eval_pred_curve(
  fit_results,
  vary_params = NULL,
  nested_cols = NULL,
  truth_col,
  prob_cols,
  group_cols = NULL,
  curve = c("ROC", "PR"),
  na_rm = FALSE
)

summarize_pred_curve(
  fit_results,
  vary_params = NULL,
  nested_cols = NULL,
  truth_col,
  prob_cols,
  group_cols = NULL,
  curve = c("ROC", "PR"),
  na_rm = FALSE,
  x_grid = seq(0, 1, by = 0.01),
  summary_funs = c("mean", "median", "min", "max", "sd", "raw"),
  custom_summary_funs = NULL,
  eval_id = ifelse(curve == "PR", "precision", "TPR")
)
}
\arguments{
\item{fit_results}{A tibble, as returned by the \code{fit} method.}

\item{vary_params}{A vector of parameter names that are varied across in the
\code{Experiment}.}

\item{nested_cols}{(Optional) A character string or vector specifying the
name of the column(s) in \code{fit_results} that need to be
unnested before evaluating results. Default is \code{NULL}, meaning no
columns in \code{fit_results} need to be unnested prior to computation.}

\item{truth_col}{A character string identifying the column with the true
responses. The column should be numeric for a regression problem and a
factor for a classification problem.}

\item{prob_cols}{A character string or vector identifying the column(s)
containing class probabilities. If the \code{truth_col} column is binary,
only 1 column name should be provided. Otherwise, the length of the
\code{prob_cols} should be equal to the number of factor levels of
the \code{truth_col} column. This argument is not used when evaluating
numeric metrics.}

\item{group_cols}{(Optional) A character string or vector specifying the
column(s) to group rows by before evaluating metrics.
This is useful for assessing within-group metrics.}

\item{curve}{Either "ROC" or "PR" indicating whether to evaluate the ROC or
Precision-Recall curve.}

\item{na_rm}{A \code{logical} value indicating whether \code{NA} values
should be stripped before the computation proceeds.}

\item{x_grid}{Vector of values between 0 and 1 at which to evaluate the ROC
or PR curve. If \code{curve = "ROC"}, the provided vector of values are
the FPR values at which to evaluate the TPR, and if \code{curve = "PR"},
the values are the recall values at which to evaluate the precision.}

\item{summary_funs}{Character vector specifying how to summarize
evaluation metrics. Must choose from a built-in library of summary
functions - elements of the vector must be one of "mean", "median",
"min", "max", "sd", "raw".}

\item{custom_summary_funs}{Named list of custom functions to summarize
results. Names in the list should correspond to the name of the summary
function. Values in the list should be a function that takes in one
argument, that being the values of the evaluated metrics.}

\item{eval_id}{Character string. ID to be used as a suffix when naming result
columns. Default \code{NULL} does not add any ID to the column names.}
}
\value{
The output of \code{eval_pred_curve()} is a \code{tibble} with the following
columns:
\describe{
\item{.rep}{Replicate ID.}
\item{.dgp_name}{Name of DGP.}
\item{.method_name}{Name of Method.}
\item{curve_estimate}{A list of tibbles with x and y coordinate values for
the ROC/PR curve for the given experimental replicate. If
\code{curve = "ROC"}, the \code{tibble} has the columns \code{.threshold},
\code{FPR}, and \code{TPR} for the threshold, false positive rate, and true
positive rate, respectively. If \code{curve = "PR"}, the \code{tibble} has
the columns \code{.threshold}, \code{recall}, and \code{precision}.}
}
as well as any columns specified by \code{group_cols} and \code{vary_params}.

The output of \code{summarize_pred_curve()} is a grouped \code{tibble}
containing both identifying information and the prediction curve results
aggregated over experimental replicates. Specifically, the identifier columns
include \code{.dgp_name}, \code{.method_name}, and any columns specified by
\code{group_cols} and \code{vary_params}. In addition, there are results
columns corresponding to the requested statistics in \code{summary_funs} and
\code{custom_summary_funs}. If \code{curve = "ROC"}, these results columns
include \code{FPR} and others that end in the suffix "_TPR". If
\code{curve = "PR"}, the results columns include \code{recall} and others
that end in the suffix "_precision".
}
\description{
Evaluate the ROC or PR curves, given the true responses and the
predicted probabilities for each class. \code{eval_pred_curve()} evaluates
the ROC or PR curve for each experimental replicate separately.
\code{summarize_pred_curve()} summarizes the ROC or PR curve across
experimental replicates.
}
\examples{
#######################################
#### Binary Classification Problem ####
#######################################
# generate example fit_results data for a binary classification problem
fit_results <- tibble::tibble(
  .rep = rep(1:2, times = 2),
  .dgp_name = c("DGP1", "DGP1", "DGP2", "DGP2"),
  .method_name = c("Method"),
  # true response
  y = lapply(1:4,
             FUN = function(x) {
               as.factor(sample(0:1, size = 100, replace = TRUE))
             }),
  # predicted class probabilities
  class_probs = lapply(1:4, FUN = function(x) runif(n = 100, min = 0, max = 1))
)

# evaluate ROC/PR curve for each replicate
roc_results <- eval_pred_curve(fit_results, curve = "ROC",
                               truth_col = "y", prob_cols = "class_probs")
pr_results <- eval_pred_curve(fit_results, curve = "PR",
                              truth_col = "y", prob_cols = "class_probs")

# summarize ROC/PR curves across replicates
roc_summary <- summarize_pred_curve(fit_results, curve = "ROC",
                                    truth_col = "y", prob_cols = "class_probs")
pr_summary <- summarize_pred_curve(fit_results, curve = "PR",
                                   truth_col = "y", prob_cols = "class_probs")

############################################
#### Multi-class Classification Problem ####
############################################
# generate example fit_results data for a multi-class classification problem
fit_results <- tibble::tibble(
  .rep = rep(1:2, times = 2),
  .dgp_name = c("DGP1", "DGP1", "DGP2", "DGP2"),
  .method_name = c("Method"),
  # true response
  y = lapply(1:4,
             FUN = function(x) {
               as.factor(sample(c("a", "b", "c"), size = 100, replace = TRUE))
             }),
  # predicted class probabilities
  class_probs = lapply(1:4,
                       FUN = function(x) {
                         tibble::tibble(a = runif(n = 100, min = 0, max = 0.5),
                                        b = runif(n = 100, min = 0, max = 0.5),
                                        c = 1 - a - b)
                       })
)

# evaluate ROC/PR curve for each replicate
roc_results <- eval_pred_curve(fit_results, curve = "ROC",
                               nested_cols = "class_probs",
                               truth_col = "y",
                               prob_cols = c("a", "b", "c"))
pr_results <- eval_pred_curve(fit_results, curve = "PR",
                              nested_cols = "class_probs",
                              truth_col = "y",
                              prob_cols = c("a", "b", "c"))

# summarize ROC/PR curves across replicates
roc_summary <- summarize_pred_curve(fit_results, curve = "ROC",
                                    nested_cols = "class_probs",
                                    truth_col = "y",
                                    prob_cols = c("a", "b", "c"))
pr_summary <- summarize_pred_curve(fit_results, curve = "PR",
                                   nested_cols = "class_probs",
                                   truth_col = "y",
                                   prob_cols = c("a", "b", "c"))

}
\seealso{
Other prediction_error_funs: 
\code{\link{eval_pred_err_funs}},
\code{\link{plot_pred_curve}()},
\code{\link{plot_pred_err}()}
}
\concept{prediction_error_funs}
