% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluator-lib-prediction-metrics.R
\name{summarize_pred_curve}
\alias{summarize_pred_curve}
\title{Summarize ROC/PR curves.}
\usage{
summarize_pred_curve(
  fit_results,
  vary_params = NULL,
  truth,
  probs,
  metric = c("ROC", "PR"),
  groups = NULL,
  options = list(),
  na_rm = FALSE,
  x_grid = seq(0, 1, by = 0.01),
  summary_funs = c("mean", "median", "min", "max", "sd", "raw"),
  custom_summary_funs = NULL
)
}
\arguments{
\item{fit_results}{A tibble, as returned by the \code{fit} method.}

\item{vary_params}{A vector of parameter names that are varied across in the
\code{Experiment}.}

\item{truth}{A character string identifying the column with the true
responses. The column should be numeric for a regression problem and a
factor for a classification problem.}

\item{probs}{A character string or vector identifying the column(s) with the
columns containing class probabilities. If \code{truth} is binary, only
1 column name should be provided. Otherwise, the length of the character
vector should be equal to the number of factor levels of \code{truth}.
This argument is not used when evaluating numeric metrics.}

\item{metric}{Either "ROC" or "PR" indicating whether to evaluate the ROC or
Precision-Recall curve.}

\item{groups}{(Optional) vector of group IDs to group observations by before
evaluating prediction errors. This is useful for assessing within-group
prediction errors. Note: the (unstratified) prediction errors, aggregated
across the full data set, are computed in addition to these stratified
within-group errors.}

\item{options}{A \code{list} of named options to pass to \code{\link[pROC:roc]{pROC::roc()}}
such as \code{smooth}. These options should not include \code{response},
\code{predictor}, \code{levels}, \code{quiet}, or \code{direction}.}

\item{na_rm}{A \code{logical} value indicating whether \code{NA} values
should be stripped before the computation proceeds.}

\item{x_grid}{Vector of values between 0 and 1 at which to evaluate the ROC
or PR curve. If \code{metric = "ROC"}, the provided vector of values are
the FPR values at which to evaluate the TPR, and if \code{metric = "PR"},
the values are the recall values at which to evaluate the precision.}

\item{summary_funs}{Character vector specifying how to summarize
evaluation metrics. Must choose from a built-in library of summary
functions - elements of the vector must be one of "mean", "median",
"min", "max", "sd", "raw".}

\item{custom_summary_funs}{Named list of custom functions to summarize
results. Names in the list should correspond to the name of the summary
function. Values in the list should be a function that takes in one
argument, that being the values of the evaluated metrics.}
}
\value{
A grouped \code{tibble} containing both identifying information
and the feature recovery curve results aggregated over experimental
replicates. Specifically, the identifier columns include \code{dgp_name},
\code{method_name}, and any columns specified by \code{vary_params}. In
addition, there are results columns corresponding to the
requested statistics in \code{summary_funs} and \code{custom_summary_funs}.
If \code{metric = "ROC"}, these results columns include \code{FPR} and
others that end in the suffix "_TPR". If \code{metric = "PR"}, the results
columns include \code{recall} and others that end in the suffix
"_precision".
}
\description{
Summarize ROC/PR curves across experimental repetitions.
}
\seealso{
Other prediction_error_funs: 
\code{\link{eval_pred_curve}()},
\code{\link{eval_pred_err}()},
\code{\link{plot_pred_curve}()},
\code{\link{plot_pred_err}()},
\code{\link{summarize_pred_err}()}
}
\concept{prediction_error_funs}
