% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluator-lib-prediction-metrics.R
\name{eval_pred_err}
\alias{eval_pred_err}
\title{Evaluate prediction error.}
\usage{
eval_pred_err(
  y,
  yhat,
  metrics = NULL,
  custom_metrics = NULL,
  group = NULL,
  na.rm = F
)
}
\arguments{
\item{y}{Vector, matrix, or data.frame of the true/observed response values.}

\item{yhat}{Vector, matrix, or data.frame of the predicted response values.
Must be of the same dimension as \code{y}.}

\item{metrics}{Character vector of the prediction error metrics to compute.
Elements of the vector must be one of "RMSE", "MSE", "R2", "MAE",
"Corr", "ClassErr", "BalancedClassErr", "AUROC", "AUPRC".}

\item{custom_metrics}{Named list of custom metric functions to compute. Names
in the list should correspond to the name of the metric. Values in the
list should be a function that takes in the arguments \code{y} and
\code{yhat} only and returns the evaluated metric value.}

\item{group}{(Optional) vector of group ids (as a factor) to use for
assessing within-group prediction errors.}

\item{na.rm}{Logical. Should missing values be removed?}
}
\value{
A (long-shaped) data frame with the following columns:
\describe{
\item{group}{Name of group for within-group prediction errors. Note that the
group id "all" is used to denote the full data set without any grouping.
Column is only present if \code{group} is not \code{NULL}.}
\item{metric}{Name of prediction error metric.}
\item{value}{Prediction error value for the given group and metric.}
}
}
\description{
Evaluate various prediction error metrics, given the observed
values \code{y} and the predicted values \code{yhat}.
}
\details{
TODO: explain different metrics and which are used for regression
vs. binary vs. classification.
}
\examples{
eval_out <- eval_pred_err(y = rnorm(10), yhat = rnorm(10),
                          metrics = c("RMSE", "MSE", "R2", "MAE", "Corr"))
eval_out <- eval_pred_err(y = rep(0:1, length.out = 10), yhat = runif(10),
                          metrics = c("AUROC", "AUPRC"))
eval_out <- eval_pred_err(y = rep(c("a", "b"), length.out = 10),
                          yhat = runif(10), metrics = c("AUROC", "AUPRC"))
eval_out <- eval_pred_err(y = rep(c("a", "b"), length.out = 10),
                          yhat = rep(c("a", "b"), length.out = 10), 
                          metrics = c("BalancedClassErr", "ClassErr"))
eval_out <- eval_pred_err(y = rnorm(10), yhat = rnorm(10),
                          metrics = c("RMSE", "MSE", "R2", "MAE", "Corr"),
                          custom_metrics = list(
                            MSE2 = function(y, yhat) {mean((y - yhat)^2)},
                            MAE2 = function(y, yhat) {mean(abs(y - yhat))}
                          ))
}
