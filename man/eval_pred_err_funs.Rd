% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluator-lib-prediction-metrics.R
\name{eval_pred_err_funs}
\alias{eval_pred_err_funs}
\alias{eval_pred_err}
\alias{summarize_pred_err}
\title{Evaluate and/or summarize prediction errors.}
\usage{
eval_pred_err(
  fit_results,
  vary_params = NULL,
  nested_data = NULL,
  truth_col,
  estimate_col,
  prob_cols = NULL,
  metrics = NULL,
  groups = NULL,
  options = list(),
  na_rm = FALSE
)

summarize_pred_err(
  fit_results,
  vary_params = NULL,
  nested_data = NULL,
  truth_col,
  estimate_col,
  prob_cols = NULL,
  metrics = NULL,
  groups = NULL,
  options = list(),
  na_rm = FALSE,
  summary_funs = c("mean", "median", "min", "max", "sd", "raw"),
  custom_summary_funs = NULL
)
}
\arguments{
\item{fit_results}{A tibble, as returned by the \code{fit} method.}

\item{vary_params}{A vector of parameter names that are varied across in the
\code{Experiment}.}

\item{nested_data}{(Optional) Character string. If specified, should be the
name of the column in \code{fit_results} containing columns that must be
unnested before evaluating results. Default is \code{NULL}, meaning no
columns in \code{fit_results} need to be unnested prior to computation.}

\item{truth_col}{A character string identifying the column with the true
responses. The column should be numeric for a regression problem and a
factor for a classification problem.}

\item{estimate_col}{A character string identifying the column with the
estimated or predicted responses. The column should be numeric for a
regression problem and a factor (with the predicted classes) for a
classification problem.}

\item{prob_cols}{A character string or vector identifying the column(s)
containing class probabilities. If the \code{truth_col} column is binary,
only 1 column name should be provided. Otherwise, the length of the
\code{prob_cols} should be equal to the number of factor levels of
the \code{truth_col} column. This argument is not used when evaluating
numeric metrics.}

\item{metrics}{A \code{metric_set} object indicating the metrics to evaluate.
See \code{\link[yardstick:metric_set]{yardstick::metric_set()}} for more details. Default \code{NULL} will
use the default metrics in \code{\link[yardstick:metrics]{yardstick::metrics()}}.}

\item{groups}{(Optional) vector of group IDs to group observations by before
evaluating prediction errors. This is useful for assessing within-group
prediction errors. Note: the (unstratified) prediction errors, aggregated
across the full data set, are computed in addition to these stratified
within-group errors.}

\item{options}{A list of named options to pass to \code{pROC::roc()} such as
\code{smooth}. These options should not include \code{response},
\code{predictor}, \code{levels}, \code{quiet}, or \code{direction}. This
argument is only used when computing the ROC and is ignored otherwise.}

\item{na_rm}{A \code{logical} value indicating whether \code{NA} values
should be stripped before the computation proceeds.}

\item{summary_funs}{Character vector specifying how to summarize
evaluation metrics. Must choose from a built-in library of summary
functions - elements of the vector must be one of "mean", "median",
"min", "max", "sd", "raw".}

\item{custom_summary_funs}{Named list of custom functions to summarize
results. Names in the list should correspond to the name of the summary
function. Values in the list should be a function that takes in one
argument, that being the values of the evaluated metrics.}
}
\value{
The output of \code{eval_pred_err()} is a \code{tibble} with the following
columns:
\describe{
\item{rep}{Replicate ID.}
\item{dgp_name}{Name of DGP.}
\item{method_name}{Name of Method.}
\item{.group}{If \code{groups} is not \code{NULL}, this column specifies the
name of the group under evaluation. Otherwise, this column is not returned.}
\item{.metric}{Name of the evaluation metric.}
\item{.estimate}{Value of the evaluation metric.}
}
as well as any columns specified by \code{vary_params}.

The output of \code{summarize_pred_err()} is a grouped \code{tibble}
containing both identifying information and the prediction error results
aggregated over experimental replicates. Specifically, the identifier columns
include \code{dgp_name}, \code{method_name}, any columns specified by
\code{vary_params}, and  \code{.metric}. In addition, there are results
columns corresponding to the requested statistics in \code{summary_funs} and
\code{custom_summary_funs}. These columns end in the suffix "_pred_err".
}
\description{
Evaluate various prediction error metrics, given the true
responses and the predicted (or estimated) responses.
\code{eval_pred_err()} evaluates the various prediction error metrics for
each experimental replicate separately. \code{summarize_pred_err()}
summarizes the various prediction error metrics across experimental
replicates.
}
\seealso{
Other prediction_error_funs: 
\code{\link{eval_pred_curve_funs}},
\code{\link{plot_pred_curve}()},
\code{\link{plot_pred_err}()}
}
\concept{prediction_error_funs}
