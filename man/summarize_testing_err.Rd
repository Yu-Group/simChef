% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluator-lib-inference.R
\name{summarize_testing_err}
\alias{summarize_testing_err}
\title{Summarize error metrics when conducting multiple tests.}
\usage{
summarize_testing_err(
  fit_results,
  vary_params = NULL,
  data_col,
  truth,
  p_value,
  metrics = NULL,
  alphas = 0.05,
  na_rm = FALSE,
  summary_funs = c("mean", "median", "min", "max", "sd", "raw"),
  custom_summary_funs = NULL
)
}
\arguments{
\item{fit_results}{A tibble, as returned by the \code{fit} method.}

\item{vary_params}{A vector of parameter names that are varied across in the
\code{Experiment}.}

\item{data_col}{(Optional) Character string specifying the name of the
list-type column in \code{fit_results} with the feature support
\code{data.frames} or \code{tibbles} that contain the \code{truth},
\code{feature}, and \code{p_value} columns. Each row in these
\code{data.frames} or \code{tibbles} typically corresponds to a feature or
variable. Default \code{NULL} is used in the case where the p-value column
is a column in \code{fit_results} and no feature information is needed.}

\item{truth}{A character string identifying the column with the true
support. The column should be binary, where \code{TRUE} or \code{1} means
the feature (corresponding to that row) is in the support and \code{FALSE}
or \code{0} means the feature is not in the support.}

\item{p_value}{A character string identifying the column with the estimated
p-values.}

\item{metrics}{A \code{metric_set} object indicating the metrics to evaluate.
See \link[yardstick:metric_set]{yardstick::metric_set} for more details. Default \code{NULL} will
evaluate the following: number of true positives (\code{tp}), number of
false positives (\code{fp}), sensitivity (\code{sens}), specificity
(\code{spec}), positive predictive value (\code{ppv}), number of features
in the estimated support (\code{pos}), number of features not in the
estimated support (\code{neg}), AUROC (\code{roc_auc}), and AUPRC
(\code{pr_auc}).}

\item{alphas}{(Optional) Vector of significance levels at which to evaluate
the various metrics. By default, \code{alphas = 0.05}.}

\item{na_rm}{A \code{logical} value indicating whether \code{NA} values
should be stripped before the computation proceeds.}

\item{summary_funs}{Character vector specifying how to summarize
evaluation metrics. Must choose from a built-in library of summary
functions - elements of the vector must be one of "mean", "median",
"min", "max", "sd", "raw".}

\item{custom_summary_funs}{Named list of custom functions to summarize
results. Names in the list should correspond to the name of the summary
function. Values in the list should be a function that takes in one
argument, that being the values of the evaluated metrics.}
}
\value{
A grouped \code{tibble} containing both identifying information
and the feature recovery results aggregated over experimental replicates.
Specifically, the identifier columns include \code{dgp_name},
\code{method_name}, any columns specified by \code{vary_params}, and
\code{.metric}. In addition, there are results columns corresponding to the
requested statistics in \code{summary_funs} and \code{custom_summary_funs}.
These columns end in the suffix "_testing_err".
}
\description{
Summarize testing error evaluation results for a variety of
evaluation metrics across experimental repetitions.
}
\seealso{
Other inference_funs: 
\code{\link{eval_pval_ranking_curve}()},
\code{\link{eval_reject_prob}()},
\code{\link{eval_testing_err}()},
\code{\link{plot_pval_ranking_curve}()},
\code{\link{plot_reject_prob}()},
\code{\link{plot_testing_err}()},
\code{\link{summarize_pval_ranking_curve}()}
}
\concept{inference_funs}
